import csv
import os
import pickle
import sys
import time
from datetime import date

import numpy as np

# Offline Gradient Updates Scheduler
# Virtual environment
# from maze3D_new.assets import *
# to track memory leaks
from pympler.tracker import SummaryTracker
from tqdm import tqdm

# Game utility file
from game.utils import (
    get_agent_only_action,
    get_distance_traveled,
    get_env_action,
)


def load_csv(file_path):
    """
    Load a csv file
    :param file_path: path to the csv file
    :return: the data in the csv file
    """
    data = []
    with open(file_path, newline="") as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            data.append(row)
    return data


tracker = SummaryTracker()


class Experiment:
    def __init__(
        self,
        environment,
        agent=None,
        load_models=False,
        config=None,
        participant_name=None,
        second_agent=None,
    ):
        # retrieve parameters
        self.config = config  # configuration file dictionary
        self.env = environment  # environment to play in
        if self.config["Experiment"]["mode"] != "human":
            self.agent = agent  # the agent to play with
            if second_agent is not None:
                self.second_agent = second_agent

            if self.config["SAC"]["load_checkpoint"]:
                self.agent.load_models()
            if self.config["SAC"]["load_second_agent"]:
                self.second_agent.load_models()
                # states = load_csv('results/SavedData/states.csv')
                # print(states)
                # actions = load_csv('results/SavedData/actions.csv')

                # self.second_agent.supervised_learn(states,actions,2500)
            # create a gradient updates scheduler
            # self.scheduler = UpdatesScheduler()

            self.isAgent_discrete = (
                config["SAC"]["discrete"] if "SAC" in config.keys() else None
            )

        # retrieve information from the config file
        self.goal = config["game"]["goal"]
        self.mode = config["Experiment"]["mode"]
        self.path_to_save = "results/" + self.mode + "/" + participant_name
        self.participant_name = participant_name

        self.max_blocks = config["Experiment"][self.mode]["max_blocks"]
        self.action_duration = config["Experiment"][self.mode][
            "action_duration"
        ]
        self.max_game_duration = config["Experiment"][self.mode][
            "max_duration"
        ]
        self.max_blocks = config["Experiment"][self.mode]["max_blocks"]
        self.log_interval = self.config["Experiment"][self.mode][
            "log_interval"
        ]

        self.second_human = (
            config["game"]["second_human"] if "game" in config.keys() else None
        )

        self.games_per_block = config["Experiment"][self.mode][
            "games_per_block"
        ]

        self.popup_window_time = config["GUI"]["popup_window_time"]

        self.best_game_score = 0
        self.last_score = 0

        self.block_number_for_rundom = 0
        self.human_actions, self.update_cycles = None, None
        self.save_models, self.flag = True, True
        self.duration_pause_total = 0

    def save_pickle(self, pt_name, data, baseline=False, game_mode="train"):
        name_of_file = (
            self.participant_name
            + "_"
            + self.mode
            + "_"
            + str(date.today())
            + ".pickle"
        )
        addintional_part = 0

        if baseline:
            mode = "baseline"
            while os.path.isfile(
                os.path.join("results", self.mode, game_mode, name_of_file)
            ):
                name_of_file = (
                    self.participant_name
                    + "_"
                    + mode
                    + "_"
                    + str(date.today())
                    + "_"
                    + str(addintional_part)
                    + ".pickle"
                )
                addintional_part += 1

            if not os.path.exists("results"):
                os.makedirs("results")
            if not os.path.exists(os.path.join("results", mode)):
                os.makedirs(os.path.join("results", mode))
            if not os.path.exists(os.path.join("results", mode, game_mode)):
                os.makedirs(os.path.join("results", mode, game_mode))

            with open(
                os.path.join("results", mode, game_mode, name_of_file), "wb"
            ) as handle:
                pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)

        else:
            while os.path.isfile(
                os.path.join("results", self.mode, game_mode, name_of_file)
            ):
                name_of_file = (
                    self.participant_name
                    + "_"
                    + self.mode
                    + "_"
                    + str(date.today())
                    + "_"
                    + str(addintional_part)
                    + ".pickle"
                )
                addintional_part += 1

            if not os.path.exists("results"):
                os.makedirs("results")
            if not os.path.exists(os.path.join("results", self.mode)):
                os.makedirs(os.path.join("results", self.mode))
            if not os.path.exists(
                os.path.join("results", self.mode, game_mode)
            ):
                os.makedirs(os.path.join("results", self.mode, game_mode))

            with open(
                os.path.join("results", self.mode, game_mode, name_of_file),
                "wb",
            ) as handle:
                pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def mz_experiment(self, participant_name):
        train_block_metrics_dict = {}
        test_block_metrics_dict = {}
        # self.maze_game_random_agent(0,10)
        for i_block in range(self.max_blocks):
            print("Test Block: ", i_block)
            test_block_metrics_dict["block_" + str(i_block)] = {}
            test_block_metrics_dict = self.maze_game(
                test_block_metrics_dict,
                "block_" + str(i_block),
                int(self.games_per_block / 2),
                i_block,
                "test",
            )
            print("Train Block: ", i_block)
            train_block_metrics_dict["block_" + str(i_block)] = {}
            train_block_metrics_dict = self.maze_game(
                train_block_metrics_dict,
                "block_" + str(i_block),
                int(self.games_per_block / 2),
                i_block,
                "train",
            )

        # Final Testing block
        print("Test Block: ", self.max_blocks)
        test_block_metrics_dict["block_" + str(self.max_blocks)] = {}
        test_block_metrics_dict = self.maze_game(
            test_block_metrics_dict,
            "block_" + str(self.max_blocks),
            int(self.games_per_block / 2),
            self.max_blocks,
            "test",
        )

        # Save to pickle file
        self.save_pickle(
            self.participant_name, train_block_metrics_dict, game_mode="train"
        )
        self.save_pickle(
            self.participant_name, test_block_metrics_dict, game_mode="test"
        )

        if not os.path.exists(
            os.path.join("results", self.mode, "buffer", self.participant_name)
        ):
            os.makedirs(
                os.path.join(
                    "results", self.mode, "buffer", self.participant_name
                )
            )

        file_name = "buffer.npy"
        c = 1
        while os.path.isfile(
            os.path.join(
                "results",
                self.mode,
                "buffer",
                self.participant_name,
                file_name,
            )
        ):
            file_name = "buffer_" + str(c) + ".npy"
            c += 1

        self.agent.memory.save_buffer(
            os.path.join(
                "results", self.mode, "buffer", self.participant_name
            ),
            file_name,
        )

    def mz_two_agents(self, participant_name):
        train_block_metrics_dict = {}
        test_block_metrics_dict = {}
        for i_block in range(self.max_blocks):
            print("Test Block: ", i_block)
            test_block_metrics_dict["block_" + str(i_block)] = {}
            test_block_metrics_dict = self.maze_two_agents(
                test_block_metrics_dict,
                "block_" + str(i_block),
                int(self.games_per_block / 2),
                i_block,
                "test",
            )
            print("Train Block: ", i_block)
            train_block_metrics_dict["block_" + str(i_block)] = {}
            train_block_metrics_dict = self.maze_two_agents(
                train_block_metrics_dict,
                "block_" + str(i_block),
                int(self.games_per_block / 2),
                i_block,
                "train",
            )

        # Final Testing block
        print("Test Block: ", self.max_blocks)
        test_block_metrics_dict["block_" + str(self.max_blocks)] = {}
        test_block_metrics_dict = self.maze_game(
            test_block_metrics_dict,
            "block_" + str(self.max_blocks),
            int(self.games_per_block / 2),
            self.max_blocks,
            "test",
        )

        # Save to pickle file
        self.save_pickle(
            self.participant_name, train_block_metrics_dict, game_mode="train"
        )
        self.save_pickle(
            self.participant_name, test_block_metrics_dict, game_mode="test"
        )

        if not os.path.exists(
            os.path.join("results", self.mode, "buffer", self.participant_name)
        ):
            os.makedirs(
                os.path.join(
                    "results", self.mode, "buffer", self.participant_name
                )
            )

        file_name = "buffer.npy"
        c = 1
        while os.path.isfile(
            os.path.join(
                "results",
                self.mode,
                "buffer",
                self.participant_name,
                file_name,
            )
        ):
            file_name = "buffer_" + str(c) + ".npy"
            c += 1

        self.agent.memory.save_buffer(
            os.path.join(
                "results", self.mode, "buffer", self.participant_name
            ),
            file_name,
        )

    def mz_only_agent(self, participant_name):
        block_metrics_dict = {}
        # self.maze_game_random_agent(0,10)
        for i_block in range(self.max_blocks):
            block_metrics_dict["block_" + str(i_block)] = {}
            block_metrics_dict = self.maze_only_agent(
                block_metrics_dict,
                "block_" + str(i_block),
                self.games_per_block,
                i_block,
            )

        # Save to pickle file
        self.save_pickle(self.participant_name, block_metrics_dict)

    def mz_eval(self, participant_name):
        block_metrics_dict = {}
        block_metrics_dict["eval_block"] = {}
        block_metrics_dict = self.maze_game(
            block_metrics_dict, "eval_block", self.games_per_block, 0, "test"
        )

        # Save to pickle file
        self.save_pickle(self.participant_name, block_metrics_dict)

    def human_play(self, participant_name):
        block_metrics_dict = {}
        block_metrics_dict["human_block"] = {}
        block_metrics_dict = self.maze_only_human(
            block_metrics_dict, "human_block", self.games_per_block, 0, "train"
        )

        # Save to pickle file
        self.save_pickle(self.participant_name, block_metrics_dict)

    def maze_game(
        self,
        block_metrics_dict,
        block_name,
        max_games,
        block_number,
        game_mode,
    ):
        fps_history = []
        action_history = []

        block_distance_travel_history = []
        agent_states = []
        step_duration_history = []
        game_duration_history = []
        step_counter_history = []

        rewards_history = []
        state_history = []
        game_score_history = []
        done_history = []
        # Starting the Block of games for each Training/Testing period
        train_game_success_counter = 0
        for i_game in range(max_games):
            game_actions = []
            game_fps = []
            is_on_pause = True
            while is_on_pause:
                print("Game Reseting")
                (
                    prev_observation,
                    setting_up_duration,
                    is_on_pause,
                ) = self.env.reset(
                    game_mode
                )  # stores the state of the environment
                norm_prev_observation = self.normalize_state(prev_observation)
            timed_out = False  # used to check if we hit the maximum train_game_number duration
            game_reward = (
                200  # keeps track of the rewards for each train_game_number
            )
            distance_travel = 0  # keeps track of the ball's travelled distance

            game_reward_history = []
            game_state_history = []

            print("Episode: " + str(i_game))

            redundant_end_duration = (
                0  # duration in the game that is not playable by the user
            )
            step_counter = 0

            game_agent_states = []
            step_duration_game = []
            game_done = []
            # Playing the Maze game
            start_game_time = time.time()

            while True:
                step_counter += (
                    1  # keep track of the step number for each game
                )
                # print("train_step_counter {}".format(train_step_counter))

                env_agent_action, real_agent_action = self.get_agent_action(
                    norm_prev_observation, "agent", game_mode
                )
                env_agent_action = int(env_agent_action)
                # print('Agent Action:',env_agent_action,type(env_agent_action))
                # check if the game has timed out
                if (
                    time.time() - start_game_time - redundant_end_duration
                    >= self.max_game_duration
                ):
                    timed_out = True

                # Environment step
                md = (
                    str(i_game + 1)
                    + " / "
                    + str(max_games)
                    + " -- "
                    + "Last Score: "
                    + str(self.last_score)
                    + " -- "
                    + "Best Score: "
                    + str(self.best_game_score)
                )
                transition = self.env.step(
                    env_agent_action,
                    timed_out,
                    self.action_duration,
                    mode="one_agent",
                    text=md,
                )
                # print('Enviorment step')
                (
                    observation,
                    reward,
                    done,
                    fps,
                    duration_pause,
                    action_pair,
                    internet_delay,
                ) = transition
                norm_observation = self.normalize_state(observation)
                redundant_end_duration += (
                    duration_pause  # keep track of the total paused time
                )
                # print(reward,timed_out)
                # keep track of the fps
                game_fps.append(fps)

                # tracking player/agent actions
                # print(observation)
                game_actions.append(action_pair)

                # add experience to buffer
                transition_info = [block_number, i_game, step_counter]

                interaction = [
                    norm_prev_observation,
                    [real_agent_action],
                    reward,
                    norm_observation,
                    done,
                    transition_info,
                ]
                if game_mode == "train":
                    self.save_experience(interaction, block_number)

                # keep track of the rewards
                game_reward_history.append(reward)
                game_state_history.append(observation)
                game_done.append(done)

                game_reward += reward  # keep track of the total game reward

                distance_travel = get_distance_traveled(
                    distance_travel, prev_observation, observation
                )

                new_row = {
                    "prev_observation": prev_observation,
                    "real_agent_action": real_agent_action,
                    "env_agent_action": env_agent_action,
                    "human_action": action_pair[1],
                    "observation": observation,
                    "reward": reward,
                }

                game_agent_states.append(new_row)

                # calculate game duration
                step_duration = time.time() - start_game_time - duration_pause
                step_duration_game.append(step_duration)

                # set the observation for the next step
                norm_prev_observation = norm_observation
                prev_observation = observation

                if done:
                    redundant_end_duration += self.popup_window_time

                    # # goal is reached
                    if not timed_out:
                        train_game_success_counter += 1
                    else:
                        game_reward = 0
                    time.sleep(self.popup_window_time)
                    break
                # print('After break')

            print(done, game_reward)
            self.last_score = game_reward
            if game_reward > self.best_game_score:
                self.best_game_score = game_reward

            done_history.append(game_done)
            step_duration_history.append(step_duration_game)
            block_distance_travel_history.append(distance_travel)
            agent_states.append(game_agent_states)
            fps_history.append(game_fps)
            action_history.append(game_actions)
            rewards_history.append(game_reward_history)
            state_history.append(game_state_history)
            game_score_history.append(game_reward)

            # keep track of total pause duration
            end_game_time = time.time()
            game_duration = (
                end_game_time - start_game_time - redundant_end_duration
            )
            game_duration_history.append(game_duration)

            step_counter_history.append(step_counter)

            if i_game >= 2 or block_number >= 1:
                if game_mode == "train":
                    print("Start Offline Gradient Updates Session")
                    self.offline_grad_updates_session(i_game, block_number)
                    update_history = self.agent.collect_data()
                    block_metrics_dict[block_name][
                        "update_history_" + str(i_game)
                    ] = update_history

        block_metrics_dict[block_name]["fps"] = fps_history
        block_metrics_dict[block_name]["actions"] = action_history
        block_metrics_dict[block_name][
            "distance_travel"
        ] = block_distance_travel_history
        block_metrics_dict[block_name]["agent_states"] = agent_states
        block_metrics_dict[block_name]["step_duration"] = step_duration_history
        block_metrics_dict[block_name]["game_duration"] = game_duration_history
        block_metrics_dict[block_name]["step_counter"] = step_counter_history
        block_metrics_dict[block_name]["rewards"] = rewards_history
        block_metrics_dict[block_name]["states"] = state_history
        block_metrics_dict[block_name]["game_score"] = game_score_history
        block_metrics_dict[block_name]["win_loss"] = [
            train_game_success_counter,
            self.games_per_block - train_game_success_counter,
        ]
        block_metrics_dict[block_name]["done"] = done_history

        # if game_mode == 'train':
        #     print('Start Offline Gradient Updates Session')
        #     self.offline_grad_updates_session(i_game, block_number)
        #     update_history = self.agent.collect_data()
        #     block_metrics_dict[block_name]['update_history'] = update_history

        return block_metrics_dict

    def maze_two_agents(
        self,
        block_metrics_dict,
        block_name,
        max_games,
        block_number,
        game_mode,
    ):
        fps_history = []
        action_history = []

        block_distance_travel_history = []
        agent_states = []
        step_duration_history = []
        game_duration_history = []
        step_counter_history = []

        rewards_history = []
        state_history = []
        game_score_history = []
        done_history = []
        # Starting the Block of games for each Training/Testing period
        train_game_success_counter = 0
        for i_game in range(max_games):
            game_actions = []
            game_fps = []
            is_on_pause = True
            while is_on_pause:
                print("Game Reseting")
                (
                    prev_observation,
                    setting_up_duration,
                    is_on_pause,
                ) = self.env.reset(
                    game_mode
                )  # stores the state of the environment
                norm_prev_observation = self.normalize_state(prev_observation)
            timed_out = False  # used to check if we hit the maximum train_game_number duration
            game_reward = (
                200  # keeps track of the rewards for each train_game_number
            )
            distance_travel = 0  # keeps track of the ball's travelled distance

            game_reward_history = []
            game_state_history = []

            print("Episode: " + str(i_game))

            redundant_end_duration = (
                0  # duration in the game that is not playable by the user
            )
            step_counter = 0

            game_agent_states = []
            step_duration_game = []
            game_done = []
            # Playing the Maze game
            start_game_time = time.time()

            while True:
                step_counter += (
                    1  # keep track of the step number for each game
                )
                # print("train_step_counter {}".format(train_step_counter))

                env_agent_action, real_agent_action = self.get_agent_action(
                    norm_prev_observation, "agent", game_mode
                )
                (
                    env_sec_agent_action,
                    real_sec_agent_action,
                ) = self.get_agent_action(
                    norm_prev_observation, "second_agent", game_mode
                )

                # check if the game has timed out
                if (
                    time.time() - start_game_time - redundant_end_duration
                    >= self.max_game_duration
                ):
                    timed_out = True

                # Environment step
                md = (
                    str(i_game + 1)
                    + " / "
                    + str(max_games)
                    + " -- "
                    + "Last Score: "
                    + str(self.last_score)
                    + " -- "
                    + "Best Score: "
                    + str(self.best_game_score)
                )
                transition = self.env.step(
                    [env_agent_action, env_sec_agent_action],
                    timed_out,
                    self.action_duration,
                    mode="two_agents",
                    text=md,
                )
                (
                    observation,
                    reward,
                    done,
                    fps,
                    duration_pause,
                    action_pair,
                    internet_delay,
                ) = transition
                norm_observation = self.normalize_state(observation)
                redundant_end_duration += (
                    duration_pause  # keep track of the total paused time
                )
                # print(reward,timed_out)
                # keep track of the fps
                game_fps.append(fps)

                # tracking player/agent actions
                # print(observation)
                game_actions.append(action_pair)

                # add experience to buffer
                transition_info = [block_number, i_game, step_counter]
                interaction = [
                    norm_prev_observation,
                    [real_agent_action, real_sec_agent_action],
                    reward,
                    norm_observation,
                    done,
                    transition_info,
                ]
                if game_mode == "train":
                    self.save_experience(interaction, block_number)

                # keep track of the rewards
                game_reward_history.append(reward)
                game_state_history.append(observation)
                game_done.append(done)

                game_reward += reward  # keep track of the total game reward

                distance_travel = get_distance_traveled(
                    distance_travel, prev_observation, observation
                )

                new_row = {
                    "prev_observation": prev_observation,
                    "real_agent_action": real_agent_action,
                    "real_sec_agent_action": real_sec_agent_action,
                    "env_agent_action": env_agent_action,
                    "env_sec_agent_action": env_sec_agent_action,
                    "human_action": action_pair[1],
                    "observation": observation,
                    "reward": reward,
                }

                game_agent_states.append(new_row)

                # calculate game duration
                step_duration = time.time() - start_game_time - duration_pause
                step_duration_game.append(step_duration)

                # set the observation for the next step
                norm_prev_observation = norm_observation
                prev_observation = observation

                if done:
                    redundant_end_duration += self.popup_window_time

                    # # goal is reached
                    if not timed_out:
                        train_game_success_counter += 1
                    else:
                        game_reward = 0
                    time.sleep(self.popup_window_time)
                    break
                # print('After break')

            print(done, game_reward)
            self.last_score = game_reward
            if game_reward > self.best_game_score:
                self.best_game_score = game_reward

            done_history.append(game_done)
            step_duration_history.append(step_duration_game)
            block_distance_travel_history.append(distance_travel)
            agent_states.append(game_agent_states)
            fps_history.append(game_fps)
            action_history.append(game_actions)
            rewards_history.append(game_reward_history)
            state_history.append(game_state_history)
            game_score_history.append(game_reward)

            # keep track of total pause duration
            end_game_time = time.time()
            game_duration = (
                end_game_time - start_game_time - redundant_end_duration
            )
            game_duration_history.append(game_duration)

            step_counter_history.append(step_counter)

            if i_game >= 2 or block_number >= 1:
                block_metrics_dict[block_name][
                    "update_history_" + str(i_game)
                ] = {}
                if game_mode == "train":
                    print("Start Offline Gradient Updates Session")
                    self.offline_grad_updates_session(i_game, block_number)
                    update_history = self.agent.collect_data()
                    update_history = self.second_agent.collect_data()
                    block_metrics_dict[block_name][
                        "update_history_" + str(i_game)
                    ]["agent"] = update_history
                    block_metrics_dict[block_name][
                        "update_history_" + str(i_game)
                    ]["second_agent"] = update_history

            # 2. Offline gradient updates session

            # running_reward, avg_length = print_logs(self.config["game"]["verbose"],
            #                                         self.config['game']['test_model'],
            #                                         self.train_total_steps, i_game, self.train_total_steps,
            #                                         running_reward,
            #                                         avg_length, self.log_interval, avg_game_duration)
        # print('Training Session Ended')

        block_metrics_dict[block_name]["fps"] = fps_history
        block_metrics_dict[block_name]["actions"] = action_history
        block_metrics_dict[block_name][
            "distance_travel"
        ] = block_distance_travel_history
        block_metrics_dict[block_name]["agent_states"] = agent_states
        block_metrics_dict[block_name]["step_duration"] = step_duration_history
        block_metrics_dict[block_name]["game_duration"] = game_duration_history
        block_metrics_dict[block_name]["step_counter"] = step_counter_history
        block_metrics_dict[block_name]["rewards"] = rewards_history
        block_metrics_dict[block_name]["states"] = state_history
        block_metrics_dict[block_name]["game_score"] = game_score_history
        block_metrics_dict[block_name]["win_loss"] = [
            train_game_success_counter,
            self.games_per_block - train_game_success_counter,
        ]
        block_metrics_dict[block_name]["done"] = done_history

        # if game_mode == 'train':
        #     print('Start Offline Gradient Updates Session')
        #     self.offline_grad_updates_session(i_game, block_number)
        #     update_history = self.agent.collect_data()
        #     block_metrics_dict[block_name]['update_history'] = update_history

        return block_metrics_dict

    def maze_only_agent(
        self,
        block_metrics_dict,
        block_name,
        max_games,
        block_number,
        game_mode,
    ):
        fps_history = []
        action_history = []
        self.block_number_for_rundom = block_number
        print("Block Number:", block_number)
        block_distance_travel_history = []
        agent_states = []
        step_duration_history = []
        game_duration_history = []
        step_counter_history = []

        rewards_history = []
        state_history = []
        game_score_history = []
        done_history = []
        # Starting the Block of games for each Training/Testing period

        train_game_success_counter = 0
        for i_game in range(max_games):
            # vector = []
            game_actions = []
            game_fps = []
            is_on_pause = True
            while is_on_pause:
                print("Game Reseting")
                (
                    prev_observation,
                    setting_up_duration,
                    is_on_pause,
                ) = self.env.reset()  # stores the state of the environment
                norm_prev_observation = self.normalize_state(prev_observation)
                # for i in range(5):
                #     vector.append(prev_observation)
            timed_out = False  # used to check if we hit the maximum train_game_number duration
            game_reward = (
                200  # keeps track of the rewards for each train_game_number
            )
            distance_travel = 0  # keeps track of the ball's travelled distance

            game_reward_history = []
            game_state_history = []

            print("Episode: " + str(i_game))

            redundant_end_duration = (
                0  # duration in the game that is not playable by the user
            )
            step_counter = 0

            game_agent_states = []
            step_duration_game = []
            game_done = []
            # Playing the Maze game
            start_game_time = time.time()
            # vector = np.array(vector)
            while True:
                step_counter += (
                    1  # keep track of the step number for each game
                )
                # print("train_step_counter {}".format(train_step_counter))

                env_agent_action, real_agent_action = self.get_agent_action(
                    norm_prev_observation, "only_agent"
                )

                # env_sec_agent_action, real_sec_agent_action = self.get_agent_action(prev_observation, 'second_agent')

                # check if the game has timed out
                if (
                    time.time() - start_game_time - redundant_end_duration
                    >= self.max_game_duration
                ):
                    timed_out = True

                # Environment step
                md = (
                    str(i_game + 1)
                    + " / "
                    + str(max_games)
                    + " -- "
                    + "Last Score: "
                    + str(self.last_score)
                    + " -- "
                    + "Best Score: "
                    + str(self.best_game_score)
                )
                transition = self.env.step(
                    env_agent_action,
                    timed_out,
                    self.action_duration,
                    mode="two_agents",
                    text=md,
                )
                (
                    observation,
                    reward,
                    done,
                    fps,
                    duration_pause,
                    action_pair,
                    internet_delay,
                ) = transition
                norm_observation = self.normalize_state(observation)
                # new_vector = vector[1:]
                # new_vector = np.append(new_vector, [observation], axis=0)
                # print(new_vector.shape)
                redundant_end_duration += (
                    duration_pause  # keep track of the total paused time
                )
                # print(reward,timed_out)
                # keep track of the fps
                game_fps.append(fps)

                # tracking player/agent actions
                # print(observation)
                game_actions.append(action_pair)

                # add experience to buffer
                transition_info = [block_number, i_game, step_counter]

                interaction = [
                    norm_prev_observation,
                    [real_agent_action],
                    reward,
                    norm_observation,
                    done,
                    transition_info,
                ]
                if game_mode == "train":
                    self.save_experience(interaction, block_number)

                # keep track of the rewards
                game_reward_history.append(reward)
                game_state_history.append(observation)
                game_done.append(done)

                game_reward += reward  # keep track of the total game reward

                distance_travel = get_distance_traveled(
                    distance_travel, prev_observation, observation
                )

                new_row = {
                    "prev_observation": prev_observation,
                    "real_agent_action": real_agent_action,
                    "env_agent_action": env_agent_action,
                    "human_action": action_pair[1],
                    "observation": observation,
                    "reward": reward,
                }

                game_agent_states.append(new_row)

                # calculate game duration
                step_duration = time.time() - start_game_time - duration_pause
                step_duration_game.append(step_duration)

                # set the observation for the next step
                norm_prev_observation = norm_observation

                if done:
                    redundant_end_duration += self.popup_window_time

                    # # goal is reached
                    if not timed_out:
                        train_game_success_counter += 1
                    else:
                        game_reward = 0
                    time.sleep(self.popup_window_time)
                    break
                # print('After break')

            print(done, game_reward)
            self.last_score = game_reward
            if game_reward > self.best_game_score:
                self.best_game_score = game_reward

            done_history.append(game_done)
            step_duration_history.append(step_duration_game)
            block_distance_travel_history.append(distance_travel)
            agent_states.append(game_agent_states)
            fps_history.append(game_fps)
            action_history.append(game_actions)
            rewards_history.append(game_reward_history)
            state_history.append(game_state_history)
            game_score_history.append(game_reward)

            # keep track of total pause duration
            end_game_time = time.time()
            game_duration = (
                end_game_time - start_game_time - redundant_end_duration
            )
            game_duration_history.append(game_duration)

            step_counter_history.append(step_counter)

            # 2. Offline gradient updates session

            # running_reward, avg_length = print_logs(self.config["game"]["verbose"],
            #                                         self.config['game']['test_model'],
            #                                         self.train_total_steps, i_game, self.train_total_steps,
            #                                         running_reward,
            #                                         avg_length, self.log_interval, avg_game_duration)
        # print('Training Session Ended')

        block_metrics_dict[block_name]["fps"] = fps_history
        block_metrics_dict[block_name]["actions"] = action_history
        block_metrics_dict[block_name][
            "distance_travel"
        ] = block_distance_travel_history
        block_metrics_dict[block_name]["agent_states"] = agent_states
        block_metrics_dict[block_name]["step_duration"] = step_duration_history
        block_metrics_dict[block_name]["game_duration"] = game_duration_history
        block_metrics_dict[block_name]["step_counter"] = step_counter_history
        block_metrics_dict[block_name]["rewards"] = rewards_history
        block_metrics_dict[block_name]["states"] = state_history
        block_metrics_dict[block_name]["game_score"] = game_score_history
        block_metrics_dict[block_name]["win_loss"] = [
            train_game_success_counter,
            self.games_per_block - train_game_success_counter,
        ]
        block_metrics_dict[block_name]["done"] = done_history

        if game_mode == "train":
            print("Start Offline Gradient Updates Session")
            self.offline_grad_updates_session(i_game, block_number)
            update_history = self.agent.collect_data()
            block_metrics_dict[block_name]["update_history"] = update_history

        return block_metrics_dict

    def maze_only_human(
        self,
        block_metrics_dict,
        block_name,
        max_games,
        block_number,
        game_mode,
    ):
        fps_history = []
        action_history = []

        block_distance_travel_history = []
        agent_states = []
        step_duration_history = []
        game_duration_history = []
        step_counter_history = []

        rewards_history = []
        state_history = []
        game_score_history = []
        done_history = []
        # Starting the Block of games for each Training/Testing period

        train_game_success_counter = 0
        for i_game in range(max_games):
            game_actions = []
            game_fps = []
            is_on_pause = True
            while is_on_pause:
                print("Game Reseting")
                (
                    prev_observation,
                    setting_up_duration,
                    is_on_pause,
                ) = self.env.reset(game_mode)
                # norm_prev_observation = self.normalize_state(prev_observation)
            timed_out = False
            game_reward = 200
            distance_travel = 0

            game_reward_history = []
            game_state_history = []

            print("Episode: " + str(i_game))

            redundant_end_duration = 0
            step_counter = 0

            game_agent_states = []
            step_duration_game = []
            game_done = []
            # Playing the Maze game
            start_game_time = time.time()

            while True:
                step_counter += 1

                if (
                    time.time() - start_game_time - redundant_end_duration
                    >= self.max_game_duration
                ):
                    timed_out = True

                md = (
                    str(i_game + 1)
                    + " / "
                    + str(max_games)
                    + " -- "
                    + "Last Score: "
                    + str(self.last_score)
                    + " -- "
                    + "Best Score: "
                    + str(self.best_game_score)
                )
                transition = self.env.step(
                    None,
                    timed_out,
                    self.action_duration,
                    mode="human",
                    text=md,
                )
                (
                    observation,
                    reward,
                    done,
                    fps,
                    duration_pause,
                    action_pair,
                    internet_delay,
                ) = transition
                # norm_observation = self.normalize_state(observation)

                redundant_end_duration += duration_pause

                game_fps.append(fps)

                game_actions.append(action_pair)

                # keep track of the rewards
                game_reward_history.append(reward)
                game_state_history.append(observation)
                game_done.append(done)

                game_reward += reward  # keep track of the total game reward

                distance_travel = get_distance_traveled(
                    distance_travel, prev_observation, observation
                )

                new_row = {
                    "prev_observation": prev_observation,
                    "human_action_1": action_pair[0],
                    "human_action_2": action_pair[1],
                    "observation": observation,
                    "reward": reward,
                }

                game_agent_states.append(new_row)

                # calculate game duration
                step_duration = time.time() - start_game_time - duration_pause
                step_duration_game.append(step_duration)

                # set the observation for the next step
                # norm_prev_observation = norm_observation
                prev_observation = observation

                if done:
                    redundant_end_duration += self.popup_window_time

                    # # goal is reached
                    if not timed_out:
                        train_game_success_counter += 1
                    else:
                        game_reward = 0
                    time.sleep(self.popup_window_time)
                    break
                # print('After break')

            print(done, game_reward)
            self.last_score = game_reward
            if game_reward > self.best_game_score:
                self.best_game_score = game_reward

            done_history.append(game_done)
            step_duration_history.append(step_duration_game)
            block_distance_travel_history.append(distance_travel)
            agent_states.append(game_agent_states)
            fps_history.append(game_fps)
            action_history.append(game_actions)
            rewards_history.append(game_reward_history)
            state_history.append(game_state_history)
            game_score_history.append(game_reward)

            # keep track of total pause duration
            end_game_time = time.time()
            game_duration = (
                end_game_time - start_game_time - redundant_end_duration
            )
            game_duration_history.append(game_duration)

            step_counter_history.append(step_counter)

        block_metrics_dict[block_name]["fps"] = fps_history
        block_metrics_dict[block_name]["actions"] = action_history
        block_metrics_dict[block_name][
            "distance_travel"
        ] = block_distance_travel_history
        block_metrics_dict[block_name]["agent_states"] = agent_states
        block_metrics_dict[block_name]["step_duration"] = step_duration_history
        block_metrics_dict[block_name]["game_duration"] = game_duration_history
        block_metrics_dict[block_name]["step_counter"] = step_counter_history
        block_metrics_dict[block_name]["rewards"] = rewards_history
        block_metrics_dict[block_name]["states"] = state_history
        block_metrics_dict[block_name]["game_score"] = game_score_history
        block_metrics_dict[block_name]["win_loss"] = [
            train_game_success_counter,
            self.games_per_block - train_game_success_counter,
        ]
        block_metrics_dict[block_name]["done"] = done_history

        return block_metrics_dict

    def save_experience(self, interaction, block_number):
        """
        Saves an interaction (prev_observation, agent_action, reward, observation, done) to the replay buffer of
        the agent.
        :param interaction: the interaction to be stored in the Replay Buffer
        """
        if self.config["Experiment"]["agent"] == "sac":
            (
                observation,
                agent_action,
                reward,
                observation_,
                done,
                transition_info,
            ) = interaction

            # we play with the RL agent
            if not self.second_human:
                self.agent.memory.add(
                    observation,
                    agent_action[0],
                    reward,
                    observation_,
                    done,
                    transition_info,
                )

                if self.mode == "no_tl_two_agents":
                    self.second_agent.memory.add(
                        observation,
                        agent_action[1],
                        reward,
                        observation_,
                        done,
                        transition_info,
                    )

    def normalize_state(self, observation):
        norm_observation = [0] * len(observation)
        # x,y from -2/2 to -1/1
        norm_observation[0] = self.norm_feature(observation[0], -2, 2)
        norm_observation[1] = self.norm_feature(observation[1], -2, 2)
        # x,y velocity from -0/2 to 0/1
        norm_observation[2] = self.norm_feature(observation[2], -4, 4)
        norm_observation[3] = self.norm_feature(observation[3], -4, 4)
        # f,t from -30/30 to -1/1
        norm_observation[4] = self.norm_feature(observation[4], -30, 30)
        norm_observation[5] = self.norm_feature(observation[5], -30, 30)
        # # f,t velocity from -1/1 to -1/1
        norm_observation[6] = self.norm_feature(observation[6], -1.9, 1.9)
        norm_observation[7] = self.norm_feature(observation[7], -1.9, 1.9)

        for i in range(len(norm_observation)):
            if norm_observation[i] > 1.3:
                norm_observation[i] = 1.3
            elif norm_observation[i] < -1.3:
                norm_observation[i] = -1.3

        # print(norm_observation)
        # print("-----------------")
        # for i in range(len(observation)):
        #     if norm_observation[i] > 1.1 or norm_observation[i] < -1.1:
        #             print("ERROR")
        #             print('Feature:',i,'Value:',float(observation[i]))
        return np.array(norm_observation)

    def norm_feature(self, feature, min, max):
        norm_feature = 2 * ((feature - min) / (max - min)) - 1
        return norm_feature

    def get_agent_action(
        self, prev_observation, enemy_status=None, game_mode="train"
    ):
        """
        Retrieves the original action from the agent and converts it into an environment-compatible one.
        Especially discrete SAC predicts actions using a categorical distribution, so we have to map these actions into
        both negative and positive numbers.
        :param prev_observation:
        :param randomness_criterion:
        :return: the environment_compatible agent's action, the original agent's action
        """
        # if playing with the agent and not with another human

        if not self.second_human and enemy_status == "agent":
            agent_action, argmax_action = self.compute_agent_action(
                prev_observation, enemy_status
            )
            if game_mode == "train":
                env_agent_action = get_env_action(
                    agent_action, self.isAgent_discrete
                )
            elif game_mode == "test":
                env_agent_action = get_env_action(
                    argmax_action, self.isAgent_discrete
                )
                agent_action = argmax_action

        elif not self.second_human and enemy_status == "second_agent":
            # compute agent's action
            agent_action, argmax_action = self.compute_agent_action(
                prev_observation, enemy_status
            )
            if game_mode == "train":
                env_agent_action = get_env_action(
                    agent_action, self.isAgent_discrete
                )
            elif game_mode == "test":
                env_agent_action = get_env_action(
                    argmax_action, self.isAgent_discrete
                )
                agent_action = argmax_action

        elif not self.second_human and enemy_status == "random":
            # print('Here again')
            # choices available to the random agent
            choices = [0, 1, 2]
            # compute random agent's action
            agent_action = int(np.random.choice(choices))
            env_agent_action = get_env_action(
                agent_action, self.isAgent_discrete
            )

        elif not self.second_human and enemy_status == "only_agent":
            agent_action, argmax_action = self.compute_agent_action(
                prev_observation, "agent"
            )
            if game_mode == "train":
                multi_actions = get_agent_only_action(agent_action)
            elif game_mode == "test":
                multi_actions = get_agent_only_action(argmax_action)
                agent_action = argmax_action
            env_agent_action = [
                get_env_action(multi_actions[0], self.isAgent_discrete),
                get_env_action(multi_actions[1], self.isAgent_discrete),
            ]

        else:
            # get second human's action
            env_agent_action = None
            agent_action = None

        return env_agent_action, agent_action

    def compute_agent_action(self, observation, status):
        if status == "agent":
            agent_action = self.agent.actor.sample_act(observation)
            return agent_action
        elif status == "second_agent":
            agent_action = self.second_agent.actor.sample_act(observation)
            return agent_action

        return agent_action

    def offline_grad_updates_session(self, i_game, block_number):
        """
        Perform a number of offline gradient updates.
        :param i_game: current game
        :return:
        """
        start_time = time.time()
        print("off policy learning.")
        # get the number of cycles
        # if self.mode == 'train_agent':
        self.update_cycles = self.config["Experiment"][self.mode][
            "updates_per_ogu"
        ]
        # else:
        #    self.update_cycles = -1 #int(self.config['Experiment'][self.mode]['total_update_cycles']/(self.config['Experiment'][self.mode]['max_blocks'] - 1))

        if self.update_cycles > 0:
            grad_updates_duration = self.grad_updates(
                self.update_cycles, block_number
            )

            print("Saving model")
            self.agent.save_models(block_number)
            if self.mode == "no_tl_two_agents":
                print("Saving second model")
                self.second_agent.save_models(block_number)

        time_to_learn = time.time() - start_time

        return grad_updates_duration, time_to_learn

    def grad_updates(self, update_cycles, block_number):
        """
        Performs a number of offline gradient updates on the agent.
        :param update_cycles: the number of gradient updates to perform on the agent
        :return: the duration in sec of the gradient updates performed
        """
        start_grad_updates = time.time()
        end_grad_updates = 0
        misc_duration = 0

        # we play with the RL agent
        if not self.second_human:
            print("Performing {} updates".format(update_cycles))
            # print a completion bar in the terminal
            for cycle_i in tqdm(range(update_cycles), file=sys.stdout):
                if not self.config["SAC"]["freeze_agent"]:
                    (
                        policy_loss,
                        q1_loss,
                        q2_loss,
                        alpha_temp,
                    ) = self.agent.learn(block_number)
                    if cycle_i % 100 == 0:
                        print(
                            "Cycle:",
                            cycle_i,
                            "Policy Loss:",
                            policy_loss,
                            "Q1 Loss:",
                            q1_loss,
                            "Q2 Loss:",
                            q2_loss,
                            "Alpha:",
                            alpha_temp,
                        )

                if self.mode == "no_tl_two_agents":
                    if not self.config["SAC"]["freeze_second_agent"]:
                        (
                            policy_loss,
                            q1_loss,
                            q2_loss,
                            alpha_temp,
                        ) = self.second_agent.learn(block_number)

                # update the target networks
                self.agent.soft_update_target()
                if self.mode == "no_tl_two_agents":
                    self.second_agent.soft_update_target()

            end_grad_updates = time.time()

        return end_grad_updates - start_grad_updates - misc_duration

    def test_buffer(self, grand_updates):
        self.grad_updates(grand_updates, 0)
